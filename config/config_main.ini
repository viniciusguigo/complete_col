[DEFAULT]
# number of episodes per experiment
n_episodes = 30
n_max_steps = 500
# hold actions for a given number of steps
n_hold_action = 1
# action modes:
#   8 = [landing] pitch, roll, throttle, yaw;
#   7 = [landing] throttle;
#   6 = [landing] pitch, roll;
#   5 = [landing] pitch, roll, throttle;
#   4 = [col_avoidance] pitch, yaw
#   3 = [col_avoidance] pitch, roll, yaw
#   2 = [col_avoidance] pitch,roll
#   1 = [col_avoidance] roll only)
n_act_mode = 8
# action level: low (0, angles), high (1, velocity), higher (2, position)
action_level = 1
# define what reward source we are using: joystick, reward_network
reward_source = custom
# high (images) or low level (encoded images + vehicle states)
feature_level = low
# select mission: collision_avoidance, landing
mission = landing
# turn gps on/off (x and y position, with respect to initial player location)
use_gps = True
# use either sparse or dense reward
reward_function = sparse
# use or not wind
use_wind = True
# use perception module or the ground truth for the landing pad
use_perception = True

[UAS]
# camera parameters (make sure it matches the settings file)
screen_width = 320
screen_height = 240
# scale input images for processing (scale observation array)
scale_input = True
scale_factor = 0.25
# rgba, rgb, grayscale, depth
camera_mode = rgb
# if want to return observation as images or flat arrays
flat_images = False
# defines we wants to save all images and depth data while training
save_training_image_data = False

# attitude parameters
alt = -6
# when n_act_mode == 1, need to define constant action in x (radians)
const_act_x = -0.5
# when n_act_mode == 6, need to define constant landing throttle
const_act_z = 0.5

# define if the human start controlling (or the learning agent)
initial_human_control = False

[MAP]
# map size
max_x = 115
min_x = -2
